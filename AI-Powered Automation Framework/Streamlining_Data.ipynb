{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# AWS Configuration\n",
    "aws_access_key = 'YOUR_AWS_ACCESS_KEY'\n",
    "aws_secret_key = 'YOUR_AWS_SECRET_KEY'\n",
    "region_name = 'us-east-1'\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3_client = boto3.client('s3',\n",
    "                         aws_access_key_id=aws_access_key,\n",
    "                         aws_secret_access_key=aws_secret_key,\n",
    "                         region_name=region_name)\n",
    "\n",
    "glue_client = boto3.client('glue',\n",
    "                           aws_access_key_id=aws_access_key,\n",
    "                           aws_secret_access_key=aws_secret_key,\n",
    "                           region_name=region_name)\n",
    "\n",
    "# Function to load data from S3\n",
    "def load_data_from_s3(bucket_name, file_key):\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    data = pd.read_csv(obj['Body'])\n",
    "    return data\n",
    "\n",
    "# Function to process data\n",
    "def process_data(data):\n",
    "    # Example: Data cleaning and transformation\n",
    "    data.dropna(inplace=True)\n",
    "    data['processed_column'] = data['raw_column'].apply(lambda x: x.lower())\n",
    "    return data\n",
    "\n",
    "# Function to upload processed data to Redshift\n",
    "def upload_to_redshift(data, table_name, redshift_conn_str):\n",
    "    engine = create_engine(redshift_conn_str)\n",
    "    data.to_sql(table_name, engine, index=False, if_exists='replace')\n",
    "    engine.dispose()\n",
    "\n",
    "# Main Automation Workflow\n",
    "def main():\n",
    "    bucket_name = 'your-s3-bucket'\n",
    "    file_key = 'data/input_data.csv'\n",
    "    redshift_conn_str = 'postgresql+psycopg2://username:password@redshift-cluster:5439/dbname'\n",
    "\n",
    "    # Load data\n",
    "    data = load_data_from_s3(bucket_name, file_key)\n",
    "    print(\"Data loaded from S3.\")\n",
    "\n",
    "    # Process data\n",
    "    processed_data = process_data(data)\n",
    "    print(\"Data processed.\")\n",
    "\n",
    "    # Upload to Redshift\n",
    "    upload_to_redshift(processed_data, 'processed_table', redshift_conn_str)\n",
    "    print(\"Data uploaded to Redshift.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
